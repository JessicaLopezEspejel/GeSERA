

# wikiSERA

SERA (Summarization Evaluation by Relevance Analysis) is an evaluation method for automatic 
summarization proposed by [(Cohan and Goharian, 2016)](https://arxiv.org/pdf/1604.00400.pdf). Here, you can find SERA implementation and SERA++.
SERA++ is a SERA extension to evaluate summaries. 

## About this code

This code was developed with  python 3.6.7 using [Whoosh](https://whoosh.readthedocs.io/en/latest/intro.html) as search
engine library. You can install all the requirements you need to run SERA evaluation system with the following instruction:

 `
 pip install -r requirements.txt
 ` 

## How to run
### Minimal test
A command for running **wikiSERA** on candidate and reference summaries. 
```
python wikisera.py candidate_summaries/ reference_summaries/ 
```
Files in the `candidate_summaries` folder must respect the `file_name.system_number` syntax.  

Files in the `reference_summaries` folder must respect the `file_name.reference_name` syntax

Example of file names syntax (from the [TAC 2009](https://tac.nist.gov/2009/Summarization/) summarization track. 
```bash
wikiSERA/candidate$ ls 
D0901-A.M.100.A.1   D0901-A.M.100.A.2  D0901-A.M.100.A.3  D0901-A.M.100.A.4  D0901-A.M.100.A.5  D0901-A.M.100.A.6  D0901-A.M.100.A.7  D0901-A.M.100.A.8
```


<!--
```
python wikisera.py 
-index_docs_folder /path/index/txt/
-save_index_folder /path/save_index/
-reference_folder /path/models/ 
-candidate_folder /path/candidates/
-results_folder /path/results/
-refine_query raw -sera_type sera
-cut_off_point 5 -num_docs_index 10000 
-index_name name  -interval 0-2
```

***useless parameters:
index_docs_folder
save_index_folder
index_name
***optional parameters
-refine_query raw
-sera_type sera
-cut_off_point 5
-num_docs_index 10000
-interval 0-2
*** TODO: 
1) Useless and optional parameters
2) Dependencies installation script
7) File Names
1) provide a limited set of the TAC 2008 corpus
2) provide a limted set of the TAC 2009 corpus
3) provide a limited set of the Cohan 2018 corpus
4) Add GPL licenses
5) Fork to an anonymous gitlab or github repository

--> 
In the following list we explain the parameters you should use:

###### Parameters:

- `-index_docs_folder` It is the folder where the documents that are indexed are. 
The files are .txt
- `-save_index_folder` It is the folder where the index files generated by whoosh are stored
- `-reference_folder` In this folder we have the gold standard summaries
- `-candidate_folder` It is the folder where there are the candidate docs
- `-results_folder` It is the folder where the scores are stored
- `-index_name` It is the name with which the index generated by whoosh is saved
- `-cut_off_point` It is the  rank cut-off point . It is the number of documents selected from the index, most related to a query.
                   We experimented with 5 and 10 ranke check-off point.
- `-num_docs_index` It is the number of documents that are indexed
- `-interval` Interval of the number of summaries to evaluate
- `-sera_type` There are two possible values: *sera* or *dis*
- `-refine_query` It is the method by which the query is redefined. There are four options:

    1. raw - use all text without changes
    2. np - noun phrases
    3. kw - keywords
    4. plus - noun, verb, adjective

## Dataset

#### Queries
We used the summaries from [TAC 2008](https://tac.nist.gov/data/past/2008/UpdateSumm08.html) as 
queries. TAC 2008 contains 48 document sets; each set represents a different topic. 
Each topic consists of two subsets: A and B. Each subset includes ten documents. 
Subset B is the update of the documents in subset A.

#### Index dataset
 
For the indexing of documents we use two databases: [AQUAINT-2](https://catalog.ldc.upenn.edu/LDC2008T25)
and [Wikipedia]().

AQUAINT-2 is a news article collection. It contains approximately 2.5 GB of text (about 907k). The articles are from October 2004 to March 2006. 
All the articles are written in English. On the other hand, Wikipedia corpus contains approximately
1,778k of documents.  


   

